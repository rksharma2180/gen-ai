spring.application.name=SimpleVectorStoreLama
# 1. Ollama Server Location
# This assumes Ollama is running on your local machine on the default port (11434)
spring.ai.ollama.base-url=http://localhost:11434

# 2. Embedding Model (for creating vectors/embeddings)
# 'nomic-embed-text' or 'mxbai-embed-large' are good choices
#spring.ai.model.embedding=ollama
spring.ai.ollama.embedding.model=nomic-embed-text
spring.ai.ollama.embedding.options.model=nomic-embed-text
#spring.ai.ollama.embedding.options.keep-alive=5m

# 3. Chat Model (for generating answers)
# Use a capable model like Llama2, Mistral, or Llama3
spring.ai.ollama.chat.options.model=mistral
spring.ai.ollama.chat.options.temperature=0.7
spring.ai.ollama.chat.options.keep-alive=5m

# Disable auto-configuration that might trigger bean creation
spring.main.lazy-initialization=false

# Connection Settings
spring.ai.retry.max-attempts=5
spring.ai.retry.backoff.initial-interval=3s
spring.ai.retry.backoff.multiplier=2
spring.ai.retry.backoff.max-interval=30s
spring.ai.retry.on-client-errors=false

# HTTP Client Settings
spring.ai.ollama.embedding.options.timeout=60s
spring.ai.ollama.chat.options.timeout=60s

# Server
server.port=8080

# Logging
logging.level.org.springframework.ai.ollama=TRACE
logging.level.org.springframework.web.client=DEBUG
logging.level.org.springframework.ai=DEBUG
#logging.level.org.springframework.ai.ollama=DEBUG


# 4. Global Settings (for compatibility with older versions)
#spring.ai.chat.client.default-model=llama2